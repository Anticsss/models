{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "implementing_PubTabNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMyGlBeKlewlC97Mt617kE/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anticsss/models/blob/master/implementing_PubTabNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbQW4a5raxTc",
        "colab_type": "text"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo5j4Wa3a4f-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8f1e26bf-0764-4154-8cb2-ce7b72831cc5"
      },
      "source": [
        "# importing prerequisites\n",
        "import sys\n",
        "import requests\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import os\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from PIL import ImageFont, ImageDraw\n",
        "from glob import glob\n",
        "import requests\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "    import jsonLines\n",
        "except:\n",
        "    !pip install jsonlines\n",
        "    import jsonlines\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Scikit-learn includes many helpful utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Input, GlobalAveragePooling2D, Add, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFVwVNvhaPNk",
        "colab_type": "text"
      },
      "source": [
        "## Download and Extract the Dataset\n",
        "\n",
        "Since the dataset is large (~12GB), here we will be downloading a small subset of the data and extract it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om_h3UOLZm0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget -O PubTabNet.tar.gz https://dax-cdn.cdn.appdomain.cloud/dax-pubtabnet/2.0.0/pubtabnet.tar.gz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKLdaCUkkSkf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c78408b-a512-423a-ad9a-2ab3b7426d61"
      },
      "source": [
        "fname = 'examples.tar.gz'\n",
        "url = 'https://dax-cdn.cdn.appdomain.cloud/dax-pubtabnet/2.0.0/' + fname\n",
        "r = requests.get(url)\n",
        "open(fname , 'wb').write(r.content)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "386348"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHor3CXnaEDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting the dataset\n",
        "#fname=\"/content/PubTabNet.tar.gz\"\n",
        "tar = tarfile.open(fname)\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVhRh6HTaYXF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb28a8eb-6621-4e3b-b1c5-3e397ee5bfb9"
      },
      "source": [
        "# Verifying the file was extracted properly\n",
        "data_path = \"examples/\"\n",
        "path.exists(data_path)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRIzNuj7afg-",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing the Data\n",
        "\n",
        "In this section, we visualize the raw image and extract it's HTML annotation from the JSON file. \n",
        "We further render the table using Jupyter notebook's inbuilt HTML capabilities. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXKdBaczaYaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to read in tables from the annotations\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from html import escape\n",
        "\n",
        "def format_html(img):\n",
        "    ''' Formats HTML code from tokenized annotation of img\n",
        "    '''\n",
        "    html_code = img['html']['structure']['tokens'].copy()\n",
        "    to_insert = [i for i, tag in enumerate(html_code) if tag in ('<td>', '>')]\n",
        "    for i, cell in zip(to_insert[::-1], img['html']['cells'][::-1]):\n",
        "        if cell['tokens']:\n",
        "            cell = [escape(token) if len(token) == 1 else token for token in cell['tokens']]\n",
        "            cell = ''.join(cell)\n",
        "            html_code.insert(i + 1, cell)\n",
        "    html_code = ''.join(html_code)\n",
        "    html_code = '''<html>\n",
        "                   <head>\n",
        "                   <meta charset=\"UTF-8\">\n",
        "                   <style>\n",
        "                   table, th, td {\n",
        "                     border: 1px solid black;\n",
        "                     font-size: 10px;\n",
        "                   }\n",
        "                   </style>\n",
        "                   </head>\n",
        "                   <body>\n",
        "                   <table frame=\"hsides\" rules=\"groups\" width=\"100%%\">\n",
        "                     %s\n",
        "                   </table>\n",
        "                   </body>\n",
        "                   </html>''' % html_code\n",
        "\n",
        "    # prettify the html\n",
        "    soup = bs(html_code)\n",
        "    html_code = soup.prettify()\n",
        "    return html_code"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlBbn-E6aYdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading an example annotation\n",
        "with jsonlines.open('examples/PubTabNet_Examples.jsonl', 'r') as reader:\n",
        "    imgs = list(reader)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8Jb7Btge1bP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "24ecc28a-ff19-41f4-fc81-9f17035e8b61"
      },
      "source": [
        "imgs[0]['filename']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'PMC4840965_004_00.png'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RgtaJDiaYg_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a7c6c7c-30cf-442a-9ae6-c9d8119bd4fe"
      },
      "source": [
        "# Showing the raw image\n",
        "from IPython.display import Image as displayImage\n",
        "filename = imgs[0]['filename']\n",
        "displayImage(\"examples/\"+filename)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "examples/PMC4840965_004_00.png",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpm4eqEBaYko",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09d20c5e-7e9c-4a08-c70f-797e0dc74268"
      },
      "source": [
        "# Extracting the HTML for the table from the annotation\n",
        "html_string = format_html(imgs[0])\n",
        "print(html_string)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<html>\n",
            " <head>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <style>\n",
            "   table, th, td {\n",
            "                     border: 1px solid black;\n",
            "                     font-size: 10px;\n",
            "                   }\n",
            "  </style>\n",
            " </head>\n",
            " <body>\n",
            "  <table frame=\"hsides\" rules=\"groups\" width=\"100%\">\n",
            "   <thead>\n",
            "    <tr>\n",
            "     <td>\n",
            "      <b>\n",
            "       Variable\n",
            "      </b>\n",
            "     </td>\n",
            "     <td>\n",
            "      <b>\n",
            "       Hazard ratio\n",
            "      </b>\n",
            "     </td>\n",
            "     <td>\n",
            "      <b>\n",
            "       95 % CI\n",
            "      </b>\n",
            "     </td>\n",
            "     <td>\n",
            "      <b>\n",
            "       <i>\n",
            "        p\n",
            "       </i>\n",
            "       value*\n",
            "      </b>\n",
            "     </td>\n",
            "    </tr>\n",
            "   </thead>\n",
            "   <tbody>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Age (median)\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "      0.716\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      ≤69\n",
            "     </td>\n",
            "     <td>\n",
            "      1.000\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      &gt;69\n",
            "     </td>\n",
            "     <td>\n",
            "      0.839\n",
            "     </td>\n",
            "     <td>\n",
            "      0.310–2.268\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Gender\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "      0.142\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Male\n",
            "     </td>\n",
            "     <td>\n",
            "      1.000\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Female\n",
            "     </td>\n",
            "     <td>\n",
            "      0.426\n",
            "     </td>\n",
            "     <td>\n",
            "      0.152–1.190\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Type of surgery\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "      0.010\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Low anterior resection\n",
            "     </td>\n",
            "     <td>\n",
            "      1.000\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Abdominoperineal resection\n",
            "     </td>\n",
            "     <td>\n",
            "      3.140\n",
            "     </td>\n",
            "     <td>\n",
            "      0.919–10.725\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Tumor location\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "      0.710\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Upper rectum\n",
            "     </td>\n",
            "     <td>\n",
            "      1.000\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Middle rectum\n",
            "     </td>\n",
            "     <td>\n",
            "      1.267\n",
            "     </td>\n",
            "     <td>\n",
            "      0.381–4.213\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Low rectum\n",
            "     </td>\n",
            "     <td>\n",
            "      1.716\n",
            "     </td>\n",
            "     <td>\n",
            "      0.419–7.026\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Grade of differentiation\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "      0.936\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      G1\n",
            "     </td>\n",
            "     <td>\n",
            "      1.000\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      G2\n",
            "     </td>\n",
            "     <td>\n",
            "      1.933\n",
            "     </td>\n",
            "     <td>\n",
            "      0.416–3.423\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      G3\n",
            "     </td>\n",
            "     <td>\n",
            "      1.119\n",
            "     </td>\n",
            "     <td>\n",
            "      0.137–9.137\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Histologic type\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "      0.299\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Adenocarcinoma\n",
            "     </td>\n",
            "     <td>\n",
            "      1.000\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Adenocarcinoma with mucinous features\n",
            "     </td>\n",
            "     <td>\n",
            "      0.381\n",
            "     </td>\n",
            "     <td>\n",
            "      0.096–1.514\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Depth of tumor invasion\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "      0.925\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      T3\n",
            "     </td>\n",
            "     <td>\n",
            "      1.000\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      T4a\n",
            "     </td>\n",
            "     <td>\n",
            "      0.919\n",
            "     </td>\n",
            "     <td>\n",
            "      0.316–2.673\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      T4b\n",
            "     </td>\n",
            "     <td>\n",
            "      0.745\n",
            "     </td>\n",
            "     <td>\n",
            "      0.172–3.223\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      Tumor size\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "      0.329\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      ≤4 cm\n",
            "     </td>\n",
            "     <td>\n",
            "      1.000\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "     <td>\n",
            "      &gt;4 cm\n",
            "     </td>\n",
            "     <td>\n",
            "      0.594\n",
            "     </td>\n",
            "     <td>\n",
            "      0.214–1.651\n",
            "     </td>\n",
            "     <td>\n",
            "     </td>\n",
            "    </tr>\n",
            "   </tbody>\n",
            "  </table>\n",
            " </body>\n",
            "</html>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeiJJQwTaYoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "63d36333-f495-4641-98a9-826c6b4271d4"
      },
      "source": [
        "# Rendering the above HTML in Jupyter Notebook for a more readable format\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(html_string))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              " <head>\n",
              "  <meta charset=\"utf-8\"/>\n",
              "  <style>\n",
              "   table, th, td {\n",
              "                     border: 1px solid black;\n",
              "                     font-size: 10px;\n",
              "                   }\n",
              "  </style>\n",
              " </head>\n",
              " <body>\n",
              "  <table frame=\"hsides\" rules=\"groups\" width=\"100%\">\n",
              "   <thead>\n",
              "    <tr>\n",
              "     <td>\n",
              "      <b>\n",
              "       Variable\n",
              "      </b>\n",
              "     </td>\n",
              "     <td>\n",
              "      <b>\n",
              "       Hazard ratio\n",
              "      </b>\n",
              "     </td>\n",
              "     <td>\n",
              "      <b>\n",
              "       95 % CI\n",
              "      </b>\n",
              "     </td>\n",
              "     <td>\n",
              "      <b>\n",
              "       <i>\n",
              "        p\n",
              "       </i>\n",
              "       value*\n",
              "      </b>\n",
              "     </td>\n",
              "    </tr>\n",
              "   </thead>\n",
              "   <tbody>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Age (median)\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "      0.716\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      ≤69\n",
              "     </td>\n",
              "     <td>\n",
              "      1.000\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      &gt;69\n",
              "     </td>\n",
              "     <td>\n",
              "      0.839\n",
              "     </td>\n",
              "     <td>\n",
              "      0.310–2.268\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Gender\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "      0.142\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Male\n",
              "     </td>\n",
              "     <td>\n",
              "      1.000\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Female\n",
              "     </td>\n",
              "     <td>\n",
              "      0.426\n",
              "     </td>\n",
              "     <td>\n",
              "      0.152–1.190\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Type of surgery\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "      0.010\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Low anterior resection\n",
              "     </td>\n",
              "     <td>\n",
              "      1.000\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Abdominoperineal resection\n",
              "     </td>\n",
              "     <td>\n",
              "      3.140\n",
              "     </td>\n",
              "     <td>\n",
              "      0.919–10.725\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Tumor location\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "      0.710\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Upper rectum\n",
              "     </td>\n",
              "     <td>\n",
              "      1.000\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Middle rectum\n",
              "     </td>\n",
              "     <td>\n",
              "      1.267\n",
              "     </td>\n",
              "     <td>\n",
              "      0.381–4.213\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Low rectum\n",
              "     </td>\n",
              "     <td>\n",
              "      1.716\n",
              "     </td>\n",
              "     <td>\n",
              "      0.419–7.026\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Grade of differentiation\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "      0.936\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      G1\n",
              "     </td>\n",
              "     <td>\n",
              "      1.000\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      G2\n",
              "     </td>\n",
              "     <td>\n",
              "      1.933\n",
              "     </td>\n",
              "     <td>\n",
              "      0.416–3.423\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      G3\n",
              "     </td>\n",
              "     <td>\n",
              "      1.119\n",
              "     </td>\n",
              "     <td>\n",
              "      0.137–9.137\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Histologic type\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "      0.299\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Adenocarcinoma\n",
              "     </td>\n",
              "     <td>\n",
              "      1.000\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Adenocarcinoma with mucinous features\n",
              "     </td>\n",
              "     <td>\n",
              "      0.381\n",
              "     </td>\n",
              "     <td>\n",
              "      0.096–1.514\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Depth of tumor invasion\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "      0.925\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      T3\n",
              "     </td>\n",
              "     <td>\n",
              "      1.000\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      T4a\n",
              "     </td>\n",
              "     <td>\n",
              "      0.919\n",
              "     </td>\n",
              "     <td>\n",
              "      0.316–2.673\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      T4b\n",
              "     </td>\n",
              "     <td>\n",
              "      0.745\n",
              "     </td>\n",
              "     <td>\n",
              "      0.172–3.223\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      Tumor size\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "      0.329\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      ≤4 cm\n",
              "     </td>\n",
              "     <td>\n",
              "      1.000\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "     <td>\n",
              "      &gt;4 cm\n",
              "     </td>\n",
              "     <td>\n",
              "      0.594\n",
              "     </td>\n",
              "     <td>\n",
              "      0.214–1.651\n",
              "     </td>\n",
              "     <td>\n",
              "     </td>\n",
              "    </tr>\n",
              "   </tbody>\n",
              "  </table>\n",
              " </body>\n",
              "</html>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaadaE6vgR8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24ee18f9-3827-4a74-c794-c98028770581"
      },
      "source": [
        "imgs[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'filename': 'PMC4840965_004_00.png',\n",
              " 'html': {'cells': [{'bbox': [1, 4, 27, 13],\n",
              "    'tokens': ['<b>', 'V', 'a', 'r', 'i', 'a', 'b', 'l', 'e', '</b>']},\n",
              "   {'bbox': [219, 4, 260, 13],\n",
              "    'tokens': ['<b>',\n",
              "     'H',\n",
              "     'a',\n",
              "     'z',\n",
              "     'a',\n",
              "     'r',\n",
              "     'd',\n",
              "     ' ',\n",
              "     'r',\n",
              "     'a',\n",
              "     't',\n",
              "     'i',\n",
              "     'o',\n",
              "     '</b>']},\n",
              "   {'bbox': [336, 4, 363, 13],\n",
              "    'tokens': ['<b>', '9', '5', ' ', '%', ' ', 'C', 'I', '</b>']},\n",
              "   {'bbox': [456, 4, 484, 13],\n",
              "    'tokens': ['<b>',\n",
              "     '<i>',\n",
              "     'p',\n",
              "     '</i>',\n",
              "     ' ',\n",
              "     'v',\n",
              "     'a',\n",
              "     'l',\n",
              "     'u',\n",
              "     'e',\n",
              "     '*',\n",
              "     '</b>']},\n",
              "   {'bbox': [1, 17, 46, 27],\n",
              "    'tokens': ['A', 'g', 'e', ' ', '(', 'm', 'e', 'd', 'i', 'a', 'n', ')']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [456, 17, 475, 27], 'tokens': ['0', '.', '7', '1', '6']},\n",
              "   {'bbox': [8, 31, 23, 41], 'tokens': [' ', '≤', '6', '9']},\n",
              "   {'bbox': [219, 31, 238, 41], 'tokens': ['1', '.', '0', '0', '0']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [8, 45, 23, 55], 'tokens': [' ', '>', '6', '9']},\n",
              "   {'bbox': [219, 45, 238, 55], 'tokens': ['0', '.', '8', '3', '9']},\n",
              "   {'bbox': [336, 45, 376, 55],\n",
              "    'tokens': ['0', '.', '3', '1', '0', '–', '2', '.', '2', '6', '8']},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [1, 59, 26, 69], 'tokens': ['G', 'e', 'n', 'd', 'e', 'r']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [456, 59, 475, 69], 'tokens': ['0', '.', '1', '4', '2']},\n",
              "   {'bbox': [8, 73, 26, 83], 'tokens': [' ', 'M', 'a', 'l', 'e']},\n",
              "   {'bbox': [219, 73, 238, 83], 'tokens': ['1', '.', '0', '0', '0']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [8, 87, 34, 97], 'tokens': [' ', 'F', 'e', 'm', 'a', 'l', 'e']},\n",
              "   {'bbox': [219, 87, 238, 97], 'tokens': ['0', '.', '4', '2', '6']},\n",
              "   {'bbox': [336, 87, 376, 97],\n",
              "    'tokens': ['0', '.', '1', '5', '2', '–', '1', '.', '1', '9', '0']},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [1, 101, 52, 111],\n",
              "    'tokens': ['T',\n",
              "     'y',\n",
              "     'p',\n",
              "     'e',\n",
              "     ' ',\n",
              "     'o',\n",
              "     'f',\n",
              "     ' ',\n",
              "     's',\n",
              "     'u',\n",
              "     'r',\n",
              "     'g',\n",
              "     'e',\n",
              "     'r',\n",
              "     'y']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [456, 101, 475, 111], 'tokens': ['0', '.', '0', '1', '0']},\n",
              "   {'bbox': [8, 115, 82, 125],\n",
              "    'tokens': [' ',\n",
              "     'L',\n",
              "     'o',\n",
              "     'w',\n",
              "     ' ',\n",
              "     'a',\n",
              "     'n',\n",
              "     't',\n",
              "     'e',\n",
              "     'r',\n",
              "     'i',\n",
              "     'o',\n",
              "     'r',\n",
              "     ' ',\n",
              "     'r',\n",
              "     'e',\n",
              "     's',\n",
              "     'e',\n",
              "     'c',\n",
              "     't',\n",
              "     'i',\n",
              "     'o',\n",
              "     'n']},\n",
              "   {'bbox': [219, 115, 238, 125], 'tokens': ['1', '.', '0', '0', '0']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [8, 129, 101, 139],\n",
              "    'tokens': [' ',\n",
              "     'A',\n",
              "     'b',\n",
              "     'd',\n",
              "     'o',\n",
              "     'm',\n",
              "     'i',\n",
              "     'n',\n",
              "     'o',\n",
              "     'p',\n",
              "     'e',\n",
              "     'r',\n",
              "     'i',\n",
              "     'n',\n",
              "     'e',\n",
              "     'a',\n",
              "     'l',\n",
              "     ' ',\n",
              "     'r',\n",
              "     'e',\n",
              "     's',\n",
              "     'e',\n",
              "     'c',\n",
              "     't',\n",
              "     'i',\n",
              "     'o',\n",
              "     'n']},\n",
              "   {'bbox': [219, 129, 238, 139], 'tokens': ['3', '.', '1', '4', '0']},\n",
              "   {'bbox': [336, 129, 379, 139],\n",
              "    'tokens': ['0', '.', '9', '1', '9', '–', '1', '0', '.', '7', '2', '5']},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [1, 143, 51, 153],\n",
              "    'tokens': ['T',\n",
              "     'u',\n",
              "     'm',\n",
              "     'o',\n",
              "     'r',\n",
              "     ' ',\n",
              "     'l',\n",
              "     'o',\n",
              "     'c',\n",
              "     'a',\n",
              "     't',\n",
              "     'i',\n",
              "     'o',\n",
              "     'n']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [456, 143, 475, 153], 'tokens': ['0', '.', '7', '1', '0']},\n",
              "   {'bbox': [8, 157, 55, 167],\n",
              "    'tokens': [' ',\n",
              "     'U',\n",
              "     'p',\n",
              "     'p',\n",
              "     'e',\n",
              "     'r',\n",
              "     ' ',\n",
              "     'r',\n",
              "     'e',\n",
              "     'c',\n",
              "     't',\n",
              "     'u',\n",
              "     'm']},\n",
              "   {'bbox': [219, 157, 238, 167], 'tokens': ['1', '.', '0', '0', '0']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [8, 171, 58, 181],\n",
              "    'tokens': [' ',\n",
              "     'M',\n",
              "     'i',\n",
              "     'd',\n",
              "     'd',\n",
              "     'l',\n",
              "     'e',\n",
              "     ' ',\n",
              "     'r',\n",
              "     'e',\n",
              "     'c',\n",
              "     't',\n",
              "     'u',\n",
              "     'm']},\n",
              "   {'bbox': [219, 171, 238, 181], 'tokens': ['1', '.', '2', '6', '7']},\n",
              "   {'bbox': [336, 171, 376, 181],\n",
              "    'tokens': ['0', '.', '3', '8', '1', '–', '4', '.', '2', '1', '3']},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [8, 185, 49, 195],\n",
              "    'tokens': [' ', 'L', 'o', 'w', ' ', 'r', 'e', 'c', 't', 'u', 'm']},\n",
              "   {'bbox': [219, 185, 238, 195], 'tokens': ['1', '.', '7', '1', '6']},\n",
              "   {'bbox': [336, 185, 376, 195],\n",
              "    'tokens': ['0', '.', '4', '1', '9', '–', '7', '.', '0', '2', '6']},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [0, 199, 77, 209],\n",
              "    'tokens': ['G',\n",
              "     'r',\n",
              "     'a',\n",
              "     'd',\n",
              "     'e',\n",
              "     ' ',\n",
              "     'o',\n",
              "     'f',\n",
              "     ' ',\n",
              "     'd',\n",
              "     'i',\n",
              "     'f',\n",
              "     'f',\n",
              "     'e',\n",
              "     'r',\n",
              "     'e',\n",
              "     'n',\n",
              "     't',\n",
              "     'i',\n",
              "     'a',\n",
              "     't',\n",
              "     'i',\n",
              "     'o',\n",
              "     'n']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [456, 199, 475, 209], 'tokens': ['0', '.', '9', '3', '6']},\n",
              "   {'bbox': [8, 213, 19, 223], 'tokens': [' ', 'G', '1']},\n",
              "   {'bbox': [219, 213, 238, 223], 'tokens': ['1', '.', '0', '0', '0']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [8, 227, 19, 237], 'tokens': [' ', 'G', '2']},\n",
              "   {'bbox': [219, 227, 238, 237], 'tokens': ['1', '.', '9', '3', '3']},\n",
              "   {'bbox': [336, 227, 376, 237],\n",
              "    'tokens': ['0', '.', '4', '1', '6', '–', '3', '.', '4', '2', '3']},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [8, 241, 19, 251], 'tokens': [' ', 'G', '3']},\n",
              "   {'bbox': [219, 241, 238, 251], 'tokens': ['1', '.', '1', '1', '9']},\n",
              "   {'bbox': [336, 241, 376, 251],\n",
              "    'tokens': ['0', '.', '1', '3', '7', '–', '9', '.', '1', '3', '7']},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [0, 255, 51, 265],\n",
              "    'tokens': ['H',\n",
              "     'i',\n",
              "     's',\n",
              "     't',\n",
              "     'o',\n",
              "     'l',\n",
              "     'o',\n",
              "     'g',\n",
              "     'i',\n",
              "     'c',\n",
              "     ' ',\n",
              "     't',\n",
              "     'y',\n",
              "     'p',\n",
              "     'e']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [456, 255, 475, 265], 'tokens': ['0', '.', '2', '9', '9']},\n",
              "   {'bbox': [8, 269, 65, 279],\n",
              "    'tokens': [' ',\n",
              "     'A',\n",
              "     'd',\n",
              "     'e',\n",
              "     'n',\n",
              "     'o',\n",
              "     'c',\n",
              "     'a',\n",
              "     'r',\n",
              "     'c',\n",
              "     'i',\n",
              "     'n',\n",
              "     'o',\n",
              "     'm',\n",
              "     'a']},\n",
              "   {'bbox': [219, 269, 238, 279], 'tokens': ['1', '.', '0', '0', '0']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [8, 283, 142, 293],\n",
              "    'tokens': [' ',\n",
              "     'A',\n",
              "     'd',\n",
              "     'e',\n",
              "     'n',\n",
              "     'o',\n",
              "     'c',\n",
              "     'a',\n",
              "     'r',\n",
              "     'c',\n",
              "     'i',\n",
              "     'n',\n",
              "     'o',\n",
              "     'm',\n",
              "     'a',\n",
              "     ' ',\n",
              "     'w',\n",
              "     'i',\n",
              "     't',\n",
              "     'h',\n",
              "     ' ',\n",
              "     'm',\n",
              "     'u',\n",
              "     'c',\n",
              "     'i',\n",
              "     'n',\n",
              "     'o',\n",
              "     'u',\n",
              "     's',\n",
              "     ' ',\n",
              "     'f',\n",
              "     'e',\n",
              "     'a',\n",
              "     't',\n",
              "     'u',\n",
              "     'r',\n",
              "     'e',\n",
              "     's']},\n",
              "   {'bbox': [219, 283, 238, 293], 'tokens': ['0', '.', '3', '8', '1']},\n",
              "   {'bbox': [336, 283, 376, 293],\n",
              "    'tokens': ['0', '.', '0', '9', '6', '–', '1', '.', '5', '1', '4']},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [0, 297, 82, 307],\n",
              "    'tokens': ['D',\n",
              "     'e',\n",
              "     'p',\n",
              "     't',\n",
              "     'h',\n",
              "     ' ',\n",
              "     'o',\n",
              "     'f',\n",
              "     ' ',\n",
              "     't',\n",
              "     'u',\n",
              "     'm',\n",
              "     'o',\n",
              "     'r',\n",
              "     ' ',\n",
              "     'i',\n",
              "     'n',\n",
              "     'v',\n",
              "     'a',\n",
              "     's',\n",
              "     'i',\n",
              "     'o',\n",
              "     'n']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [456, 297, 475, 307], 'tokens': ['0', '.', '9', '2', '5']},\n",
              "   {'bbox': [8, 311, 18, 321], 'tokens': [' ', 'T', '3']},\n",
              "   {'bbox': [219, 311, 238, 321], 'tokens': ['1', '.', '0', '0', '0']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [8, 325, 22, 335], 'tokens': [' ', 'T', '4', 'a']},\n",
              "   {'bbox': [219, 325, 238, 335], 'tokens': ['0', '.', '9', '1', '9']},\n",
              "   {'bbox': [336, 325, 376, 335],\n",
              "    'tokens': ['0', '.', '3', '1', '6', '–', '2', '.', '6', '7', '3']},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [8, 339, 22, 349], 'tokens': [' ', 'T', '4', 'b']},\n",
              "   {'bbox': [219, 339, 238, 349], 'tokens': ['0', '.', '7', '4', '5']},\n",
              "   {'bbox': [336, 339, 376, 349],\n",
              "    'tokens': ['0', '.', '1', '7', '2', '–', '3', '.', '2', '2', '3']},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [0, 353, 37, 363],\n",
              "    'tokens': ['T', 'u', 'm', 'o', 'r', ' ', 's', 'i', 'z', 'e']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [456, 353, 475, 363], 'tokens': ['0', '.', '3', '2', '9']},\n",
              "   {'bbox': [8, 367, 32, 377], 'tokens': [' ', '≤', '4', ' ', 'c', 'm']},\n",
              "   {'bbox': [219, 367, 238, 377], 'tokens': ['1', '.', '0', '0', '0']},\n",
              "   {'tokens': []},\n",
              "   {'tokens': []},\n",
              "   {'bbox': [8, 381, 31, 391], 'tokens': [' ', '>', '4', ' ', 'c', 'm']},\n",
              "   {'bbox': [219, 381, 238, 391], 'tokens': ['0', '.', '5', '9', '4']},\n",
              "   {'bbox': [336, 381, 376, 391],\n",
              "    'tokens': ['0', '.', '2', '1', '4', '–', '1', '.', '6', '5', '1']},\n",
              "   {'tokens': []}],\n",
              "  'structure': {'tokens': ['<thead>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '</thead>',\n",
              "    '<tbody>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '<tr>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '<td>',\n",
              "    '</td>',\n",
              "    '</tr>',\n",
              "    '</tbody>']}},\n",
              " 'imgid': 0,\n",
              " 'split': 'train'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj5mdBWhHi28",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7b7ac898-543e-4dd4-8ed2-e9bece8ef44b"
      },
      "source": [
        "PATH = os.path.abspath('.') +\"/\"+ data_path\n",
        "PATH"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/examples/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXB8GsHUG8Ds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "8700ceb4-9eea-45cf-fae6-e6743100b1c9"
      },
      "source": [
        "# Store captions and image names in vectors\n",
        "all_captions = []\n",
        "all_img_name_vector = []\n",
        "\n",
        "for img in imgs:\n",
        "    caption = '<start> ' + ' '.join(img['html']['structure']['tokens']) + ' <end>'\n",
        "    image_id = img['filename']\n",
        "    full_image_path = PATH + 'examples/' + image_id\n",
        "\n",
        "    all_img_name_vector.append(full_image_path)\n",
        "    all_captions.append(caption)\n",
        "\n",
        "# Shuffle captions and image_names together\n",
        "# Set a random state\n",
        "train_captions, img_name_vector = shuffle(all_captions,\n",
        "                                          all_img_name_vector,\n",
        "                                          random_state=1)\n",
        "\n",
        "# Select the first 30000 captions from the shuffled set\n",
        "#num_examples = 30000\n",
        "#train_captions = train_captions[:num_examples]\n",
        "#img_name_vector = img_name_vector[:num_examples]\n",
        "train_captions, img_name_vector"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['<start> <thead> <tr> <td> </td> <td  colspan=\"5\" > </td> <td  colspan=\"5\" > </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td  colspan=\"6\" > </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td  colspan=\"5\" > </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td  colspan=\"2\" > </td> <td  colspan=\"3\" > </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td  colspan=\"2\" > </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td  rowspan=\"2\" > </td> <td  colspan=\"3\" > </td> <td  colspan=\"3\" > </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td  rowspan=\"2\" > </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td  rowspan=\"2\" > </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td  rowspan=\"2\" > </td> <td  colspan=\"2\" > </td> <td  colspan=\"2\" > </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td  colspan=\"4\" > </td> </tr> <tr> <td  rowspan=\"3\" > </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td  rowspan=\"3\" > </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td  rowspan=\"3\" > </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td  colspan=\"4\" > </td> </tr> <tr> <td  rowspan=\"3\" > </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td  rowspan=\"3\" > </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td  rowspan=\"3\" > </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td  colspan=\"4\" > </td> </tr> <tr> <td  rowspan=\"3\" > </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td  rowspan=\"3\" > </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td  rowspan=\"3\" > </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td  colspan=\"4\" > </td> </tr> </thead> <tbody> <tr> <td  colspan=\"4\" > </td> </tr> <tr> <td  colspan=\"4\" > </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td  colspan=\"4\" > </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td  colspan=\"4\" > </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td  colspan=\"3\" > </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td  colspan=\"3\" > </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> <end>',\n",
              "  '<start> <thead> <tr> <td> </td> <td> </td> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> </tr> </tbody> <end>'],\n",
              " ['/content/examples/examples/PMC1626454_002_00.png',\n",
              "  '/content/examples/examples/PMC4682394_003_00.png',\n",
              "  '/content/examples/examples/PMC3907710_006_00.png',\n",
              "  '/content/examples/examples/PMC5134617_013_00.png',\n",
              "  '/content/examples/examples/PMC4776821_005_00.png',\n",
              "  '/content/examples/examples/PMC2759935_007_01.png',\n",
              "  '/content/examples/examples/PMC2838834_005_00.png',\n",
              "  '/content/examples/examples/PMC4172848_007_00.png',\n",
              "  '/content/examples/examples/PMC3519711_003_00.png',\n",
              "  '/content/examples/examples/PMC4517499_004_00.png',\n",
              "  '/content/examples/examples/PMC5577841_001_00.png',\n",
              "  '/content/examples/examples/PMC4840965_004_00.png',\n",
              "  '/content/examples/examples/PMC5402779_004_00.png',\n",
              "  '/content/examples/examples/PMC5332562_005_00.png',\n",
              "  '/content/examples/examples/PMC5679144_002_01.png',\n",
              "  '/content/examples/examples/PMC4003957_018_00.png',\n",
              "  '/content/examples/examples/PMC5198506_004_00.png',\n",
              "  '/content/examples/examples/PMC3826085_003_00.png',\n",
              "  '/content/examples/examples/PMC2753619_002_00.png',\n",
              "  '/content/examples/examples/PMC5897438_004_00.png'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCLnKsDWIuZ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e923ec88-ae2b-41fd-9b34-821728dd6fde"
      },
      "source": [
        "len(train_captions), len(img_name_vector)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bGAaP8GEjGC",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53zBmeFpE4Kl",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocess the images using InceptionV3\n",
        "Next, we will use InceptionV3 (which is pretrained on Imagenet) to classify each image. We will preprocess images.\n",
        "\n",
        "First, we will reshape the images into expected format by:\n",
        "\n",
        "* Resizing the image to 448px by 448px\n",
        "\n",
        "\n",
        "*   Preprocess the images using the preprocess_input method to normalize the image so that it contains pixels in the range of -1 to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SXWY7AiyQ4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTndgO2UEtFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nioKtlTvGaTs",
        "colab_type": "text"
      },
      "source": [
        "## Caching the features extracted from InceptionV3\n",
        "\n",
        "We will normalize each channel by z-score of each image and cache the output to disk. Caching the output in RAM would be faster but also memory intensive, requiring 448 \\* 448 \\* 3 floats per image. At the time of writing, this exceeds the memory limitations of Colab (currently 12GB of memory).\n",
        "\n",
        "Performance could be improved with a more sophisticated caching strategy (for example, by sharding the images to reduce random access disk I/O), but that would require more code.\n",
        "\n",
        "The caching will take about 10 minutes to run in Colab with a GPU. If you'd like to see a progress bar, you can: \n",
        "\n",
        "1. install [tqdm](https://github.com/tqdm/tqdm):\n",
        "\n",
        "    `!pip install tqdm`\n",
        "\n",
        "2. Import tqdm:\n",
        "\n",
        "    `from tqdm import tqdm`\n",
        "\n",
        "3. Change the following line:\n",
        "\n",
        "    `for img, path in image_dataset:`\n",
        "\n",
        "    to:\n",
        "\n",
        "    `for img, path in tqdm(image_dataset):`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FY4sBeDE1CE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "4fd4bdbf-faa3-4944-b7fa-f8ea08b8f829"
      },
      "source": [
        "# Get unique images\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "# Feel free to change batch_size according to your system configuration\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(1)\n",
        "\n",
        "for img, path in image_dataset:\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  print(batch_features.shape)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "  print(batch_features.shape)\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n",
            "(1, 5, 5, 2048)\n",
            "(1, 25, 2048)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqfkvCbXFc9Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "a0a03a2b-daa5-4dd3-c496-e37fc4c6073e"
      },
      "source": [
        "\"\"\"\n",
        "# Get unique images\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "# Feel free to change batch_size according to your system configuration\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(10)\n",
        "\n",
        "for img, path in tqdm(image_dataset):\n",
        "  #print(img.shape)\n",
        "  batch_features = tf.image.per_image_standardization(img)\n",
        "  print(batch_features.shape)\n",
        "\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "  print(batch_features.shape)  \n",
        "  \n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())\n",
        "\"\"\""
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Get unique images\\nencode_train = sorted(set(img_name_vector))\\n\\n# Feel free to change batch_size according to your system configuration\\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\\nimage_dataset = image_dataset.map(\\n  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(10)\\n\\nfor img, path in tqdm(image_dataset):\\n  #print(img.shape)\\n  batch_features = tf.image.per_image_standardization(img)\\n  print(batch_features.shape)\\n\\n  batch_features = tf.reshape(batch_features,\\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\\n  print(batch_features.shape)  \\n  \\n  for bf, p in zip(batch_features, path):\\n    path_of_feature = p.numpy().decode(\"utf-8\")\\n    np.save(path_of_feature, bf.numpy())\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0nTvt9HQ611",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess and tokenize the captions\n",
        "\n",
        "* First, you'll tokenize the captions (for example, by splitting on spaces). This gives us a  vocabulary of all of the unique words in the data (for example, \"surfing\", \"football\", and so on).\n",
        "* Next, you'll limit the vocabulary size to the top 5,000 words (to save memory). You'll replace all other words with the token \"UNK\" (unknown).\n",
        "* You then create word-to-index and index-to-word mappings.\n",
        "* Finally, you pad all sequences to be the same length as the longest one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA-x4-6fJOvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the maximum length of any caption in our dataset\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpNQ99bRRCu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose the top 5000 words from the vocabulary\n",
        "top_k = 32\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxBbR44bRN6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iun93dSdRPvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the tokenized vectors\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoy_Nw4kR1rA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad each vector to the max_length of the captions\n",
        "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV-dYoi7R42N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculates the max_length, which is used to store the attention weights\n",
        "max_length = calc_max_length(train_seqs)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UOBkx1tR7l6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4dd643d-7960-411c-ae78-c1f6d7b391f2"
      },
      "source": [
        "max_length"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "869"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypYKklnSSMcV",
        "colab_type": "text"
      },
      "source": [
        "## Create a tf.data dataset for training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gbIlYYqSQ6F",
        "colab_type": "text"
      },
      "source": [
        " Our images and captions are ready! Next, let's create a tf.data dataset to use for training our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VMqpImDR9fM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feel free to change these parameters according to your system's configuration\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 16\n",
        "units = 12\n",
        "vocab_size = top_k + 1\n",
        "num_steps = len(img_name_vector) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 1"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHMoYv5UAmNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the numpy files\n",
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "  return img_tensor, cap"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0prpxCfaAm5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_vector, cap_vector))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-JYF_4dBie0",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "\n",
        "Fun fact: the decoder below is identical to the one in the example for [Neural Machine Translation with Attention](../sequences/nmt_with_attention.ipynb).\n",
        "\n",
        "The model architecture is inspired by the [Show, Attend and Tell](https://arxiv.org/pdf/1502.03044.pdf) paper.\n",
        "\n",
        "* This image is then passed through the CNN Encoder (which consists of a single Fully connected layer).\n",
        "* The RNN (here GRU) attends over the image to predict the next word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gHNu4jvA6a1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
        "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                         self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # score shape == (batch_size, 64, 1)\n",
        "    # This gives you an unnormalized score for each image feature.\n",
        "    score = self.V(attention_hidden_layer)\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No2iK2UQEsQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it using pickle\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypfHUKvAJbIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_hcXCr7Y6Dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Dcx2l6GXzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWfgnV5FGfD5",
        "colab_type": "text"
      },
      "source": [
        "## Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSotesa_Gfjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U31G6euGkOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "  # restoring the latest checkpoint in checkpoint_path\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjywM2QAGuFr",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "* You extract the features stored in the respective `.npy` files and then pass those features through the encoder.\n",
        "* The encoder output, hidden state(initialized to 0) and the decoder input (which is the start token) is passed to the decoder.\n",
        "* The decoder returns the predictions and the decoder hidden state.\n",
        "* The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "* Use teacher forcing to decide the next input to the decoder.\n",
        "* Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n",
        "* The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR_1g75NGoyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding this in a separate cell because if you run the training cell\n",
        "# many times, the loss_plot array will be reset\n",
        "loss_plot = []"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf-I6SZqGs0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "  loss = 0\n",
        "\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      features = encoder(img_tensor)\n",
        "\n",
        "      for i in range(1, target.shape[1]):\n",
        "          # passing the features through the decoder\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "  return loss, total_loss"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SwttzDGG0Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 2\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
        "                                         total_loss/num_steps))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkLp6yB7G27n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVs8uKqKHArf",
        "colab_type": "text"
      },
      "source": [
        "## Caption!\n",
        "\n",
        "* The evaluate function is similar to the training loop, except you don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the end token.\n",
        "* And store the attention weights for every time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df9AbaKiG9T1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI-6-9GnHEEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for l in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
        "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
        "        ax.set_title(result[l])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDhC3_1_HHJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# captions on the validation set\n",
        "rid = np.random.randint(0, len(img_name_val))\n",
        "image = img_name_val[rid]\n",
        "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "result, attention_plot = evaluate(image)\n",
        "\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Prediction Caption:', ' '.join(result))\n",
        "plot_attention(image, result, attention_plot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQd8UvRIHZVH",
        "colab_type": "text"
      },
      "source": [
        "## Try it on your own images\n",
        "For fun, below we've provided a method you can use to caption your own images with the model we've just trained. Keep in mind, it was trained on a relatively small amount of data, and your images may be different from the training data (so be prepared for weird results!)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6hvOuqvHJrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_path = # input an image path\n",
        "\n",
        "result, attention_plot = evaluate(image_path)\n",
        "print ('Prediction Caption:', ' '.join(result))\n",
        "plot_attention(image_path, result, attention_plot)\n",
        "# opening the image\n",
        "Image.open(image_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}